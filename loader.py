#nltk.download('stopwords')
from nltk.corpus import stopwords
import string
import os
from torch.distributions import uniform
import torch
import numpy as np
import re
from pyvi import ViTokenizer
import string
from vncorenlp import VnCoreNLP
annotator = VnCoreNLP("/content/drive/My Drive/Sentiment-analysis/VnCoreNLP-master/VnCoreNLP-1.1.1.jar", annotators="wseg", max_heap_size='-Xmx500m') 

wrong_terms = {
    'Ã´ kÃªi': ' ok ', 'okie': ' ok ', 'o kÃª': ' ok ', 'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', 'oke': ' ok ',
    'okay': ' ok ', 'okÃª': ' ok ', 'ote': ' ok ',
    'kg ': u' khÃ´ng ', 'not': u' khÃ´ng ', u' kg ': u' khÃ´ng ', '"k ': u' khÃ´ng ', ' kh ': u' khÃ´ng ',
    'kÃ´': u' khÃ´ng ', 'hok': u' khÃ´ng ', ' kp ': u' khÃ´ng pháº£i ', u' kÃ´ ': u' khÃ´ng ', '"ko ': u' khÃ´ng ',
    ' cam ': ' camera ', ' cameera ': ' camera ', 'thuáº¿t káº¿': u'thiáº¿t_káº¿', 'áº¿t_káº¿ ': u' thiáº¿t_káº¿ ',
    'gud': u' tá»‘t ', 'god': u' tá»‘t ', 'wel done': ' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
    'sáº¥u': u' xáº¥u ', 'gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t',
    'bt': u' bÃ¬nh thÆ°á»ng ','sp':u'sáº£n pháº©m ',
    ' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ', 'mÃ¬n': u'mÃ¬nh', u' mÃ¬nhh ': u' mÃ¬nh ', u' má»nh ': ' mÃ¬nh ',
    ' mk ': u' mÃ¬nh ', ' mik ': ' mÃ¬nh ',
    ' ko ': u' khÃ´ng ', u' k ': u' khÃ´ng ', 'khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
    u' wÃ¡ ': u' quÃ¡ ', ' wa ': u' quÃ¡ ', u'qÃ¡': u' quÃ¡ ',
    ' cute ': 'dá»…_thÆ°Æ¡ng',
    u' táº¹c vá»i ': ' tuyá»‡t_vá»i ', u'tiá»‡c dá»i': ' tuyá»‡t_vá»i ', u'táº¹c zá»i': ' tuyá»‡t_vá»i ',
    ' dc ': u' Ä‘Æ°á»£c ', u' Ä‘c ': u' Ä‘Æ°á»£c ', ' j ': ' gÃ¬ ',
    ' mÃ n hÃ¬n ': ' mÃ n_hÃ¬nh ', u' mÃ ng hÃ¬nh ': u' mÃ n_hÃ¬nh ', ' dt ': u' Ä‘iá»‡n_thoáº¡i ',
    ' Ä‘t ': u' Ä‘iá»‡n_thoáº¡i ',
    ' tet ': u' kiá»ƒm_tra ', ' test ': u' kiá»ƒm_tra ', ' tÃ©t ': u' kiá»ƒm_tra ',
    u' cáº©m ': u' cáº§m ', u' cáº¥m ': u' cáº§m ', u' sÆ°á»›c ': u' xÆ°á»›c ', u' xÆ°á»›t ': u' xÆ°á»›c ',
    u'sÃ i ': ' xÃ i ',
    u' má»±Æ¡t ': u' mÆ°á»£t ',
    u' sá»©c sáº¯c ': u' xuáº¥t_sáº¯c ', u' xá»©c sáº¯c ': u' xuáº¥t_sáº¯c ',
    ' fai ': u' pháº£i ', u' fáº£i ': u' pháº£i ',
    u' bÃ¢y_h ': u' bÃ¢y_giá» ',
    u' má»™i ': u' má»i ', ' moi ': u' má»i ',
    u'á»£c Ä‘iá»ƒm ': u' nhÆ°á»£c Ä‘iá»ƒm ',
    u' sÃ¡msumg ': ' samsung ', ' sam ': ' samsung ', 'sam_sung ': ' samsung ',
    u' kbiáº¿t ': u' khÃ´ng_biáº¿t ', u' ráº¥t tiáº¿t ': u' ráº¥t_tiáº¿c ', u' ráº¥t_tiáº¿t ': u' ráº¥t_tiáº¿c ',
    u' ráº¥t tiÃªc ': u' ráº¥t_tiáº¿c ',
    u' lÃ¡t ': ' lag ', u' lÃ¡c ': ' lag ', ' lat ': ' lag ', ' lac ': ' lag ', u' khá»±ng ': ' lag ',
    u' giáº­t ': ' lag ',
    u' vÄƒng ra ': ' lag ', u' Ä‘Æ¡ ': ' lag ', u' láº¯c ': ' lag ',
    u' film ': ' phim ', ' phin ': ' phim ', ' fim ': ' phim ',
    ' nhung ': u' nhÆ°ng ', u' áº¥u hÃ¬nh ': u' cáº¥u_hÃ¬nh ',
    ' sd ': u' sá»­_dá»¥ng ', u' mÃ i ': u' mÃ u ', u' láº¥m ': u' láº¯m ',
    u' tÃ´t ': ' tá»‘t ', u' tÃ´n ': u' tá»‘t ', 'aple ': ' apple ', "gja": u" giÃ¡ ", u"sá»¥c": u"sá»¥t",
    u' Ã¢m_lÆ°á»£ ': u' Ã¢m_lÆ°á»£ng ', u' tháº¥t_vá» ': u' tháº¥t_vá»ng ', u' dÃ¹g ': u' dÃ¹ng ',
    u' bá»— ': u' bá»• ',
    u' sá»¥t ': u' tá»¥t ', u' tuá»™t ': u' tá»¥t ', u' xuá»‘ng ': u' tá»¥t ',
    u'chÃ­p ': ' chip ',
    ' bin ': ' pin '
}
emotion_icons = {
    "ğŸ‘¹": "negative", "ğŸ‘»": "positive", "ğŸ’ƒ": "positive",'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ',
    "ğŸ’„": "positive", "ğŸ’": "positive", "ğŸ’©": "positive","ğŸ˜•": "negative", "ğŸ˜±": "negative", "ğŸ˜¸": "positive",
    "ğŸ˜¾": "negative", "ğŸš«": "negative",  "ğŸ¤¬": "negative","ğŸ§š": "positive", "ğŸ§¡": "positive",'ğŸ¶':' positive ',
    'ğŸ‘': ' negative ', 'ğŸ˜£': ' negative ','âœ¨': ' positive ', 'â£': ' positive ','â˜€': ' positive ',
    'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' positive ',
    'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' negative ', 'ğŸ˜¢': ' negative ',
    'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' negative ', 'ğŸ˜Š': ' positive ',
    '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜­': ' negative ',
    'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',
    '^^': ' positive ', 'ğŸ˜¨': ' negative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',
    'ğŸ˜–': ' negative ', 'ğŸ˜€': ' positive ', ':((': ' negative ', 'ğŸ˜¡': ' negative ', 'ğŸ˜ ': ' negative ',
    'ğŸ˜’': ' negative ', 'ğŸ™‚': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',
    'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',
    'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜“': ' negative ', 'ï¸ğŸ†—ï¸': ' positive ',
    'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ',
    'ğŸ’“': ' positive ', 'ğŸ˜': ' negative ', ':3': ' positive ', 'ğŸ˜«': ' negative ', 'ğŸ˜¥': ' negative ',
    'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',
    'ğŸ˜—': ' positive ', 'ğŸ¤”': ' negative ', 'ğŸ˜‘': ' negative ', 'ğŸ”¥': ' negative ', 'ğŸ™': ' negative ',
    'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',
    'ğŸ˜š': ' positive ', 'âŒ': ' negative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',
    'ğŸŒ': ' positive ',  'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',
    'ğŸŒ¼': ' positive ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',
    'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ','ğŸ˜Œ':'negative',
    'ğŸ’°': ' positive ',  'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ',
    'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ', 'â˜¹': ' negative ',  'ğŸ’€': ' negative ',
    'ğŸ˜”': ' negative ', 'ğŸ˜§': ' negative ', 'ğŸ˜©': ' negative ', 'ğŸ˜°': ' negative ', 'ğŸ˜³': ' negative ',
    'ğŸ˜µ': ' negative ', 'ğŸ˜¶': ' negative ', 'ğŸ™': ' negative ', ':))': ' positive ', ':)': ' positive ',
    'he he': ' positive ', 'hehe': ' positive ', 'hihi': ' positive ', 'haha': ' positive ',
    'hjhj': ' positive ', ' lol ': ' negative ', 'huhu': ' negative ', ' 4sao ': ' positive ', ' 5sao ': ' positive ',
    ' 1sao ': ' negative ', ' 2sao ': ' negative ',
    ': ) )': ' positive ', ' : ) ': ' positive ','ğŸŒŸ':'sao','ğŸ»':"",
}
def preprecess_feature(sentence):
    # replace_wrong_terms
    for key, value in wrong_terms.items():
        sentence = sentence.replace(key, value)
    # replace_emotion_icons
    for key, value in emotion_icons.items():
        if sentence.find(key) >= 0:
            sentence = sentence.replace(key, value)
    # remove_repeated_characters
    sentence = re.sub(r'([A-Z])\1+', lambda m: m.group(1), sentence, flags=re.IGNORECASE)

    punctuation = """!"#$%&\'()*+,-./:;<=>?@[\\]^`{|}~"""
    translator = str.maketrans(punctuation, ' ' * len(punctuation))
    sentence = sentence.translate(translator)
    #sentence = annotator.tokenize(sentence)
    return sentence
def preprocess_data(lines):

    #lines = remove_special_text(lines)

    #print(lines)
    formatted_lines = []
    for tokens in lines:   
        tokens = preprecess_feature(tokens)
        #tokens = annotator.tokenize(tokens)
        #tokens_list = []
        #for sublist in tokens:
        #    for item in sublist:
        #        tokens_list.append(item)
        #print(tokens)
        tokens = tokens.split(" ")
        tokens_list = [word for word in tokens if len(word) > 1]
        #tokens = [word for word in tokens if word.isalpha()]
        stopwords = open("Vietnamese-stopword.txt", "r", encoding='utf-8')
        stop_words = set(stopwords.read().split())
        tokens = [w for w in tokens_list if w not in stop_words]
        tokens = u" ".join(tokens)
        formatted_lines.append(tokens)
    #print(len(formatted_lines))
    return formatted_lines


def load_data(dataset_path):
    """Load test, validation and train data with their labels"""
    x_train = []
    y_train = []
    x_val = []
    y_val = []
    x_test = []
    y_test = []

    with open(os.path.join(dataset_path, "train.txt"), 'r', encoding='utf-8') as f:
        for line in f.readlines():
            label = int(line[0])
            sentence = line[1:].strip()
            x_train.append(sentence)
            y_train.append(label)
    with open(os.path.join(dataset_path, "dev.txt"), 'r', encoding='utf-8') as f:
        for line in f.readlines():
            label = int(line[0])
            sentence = line[1:].strip()
            x_val.append(sentence)
            y_val.append(label)
    with open(os.path.join(dataset_path, "test.txt"), 'r', encoding='utf-8') as f:
        for line in f.readlines():
            label = int(line[0])
            sentence = line[1:].strip()
            x_test.append(sentence)
            y_test.append(label)
    #print(x_train.size(0),y_train.size(0))
    return x_train, x_val, x_test, y_train, y_val, y_test


def load_embedding(embed_path):
    """Load word embedding and return word-embedding vocabulary"""
    embedding2index = {}
    with open(embed_path, 'r', encoding='utf-8') as f:
        for line in f.readlines():
            lexicons = line.split()
            #word = lexicons[0]
            word = ''.join(lexicons[:-300])
            #print(word)
            embedding2index[word] = torch.from_numpy(np.asarray(lexicons[-300:], dtype='float64'))
        embedding_size = len(lexicons) - 1
        #print(embedding_size, embedding2index)
    return embedding2index, embedding_size


def load_embedding_matrix(embedding, words, embedding_size):
    """Add new words in the embedding matrix and return it"""
    embedding_matrix = torch.zeros(len(words), embedding_size)
    for i, word in enumerate(words):
        # Note: PAD embedded as sequence of zeros
        if word not in embedding:
            if word != 'PAD':
                embedding_matrix[i] = uniform.Uniform(-0.25, 0.25).sample(torch.Size([embedding_size]))
        else:
            embedding_matrix[i] = embedding[word]
    return embedding_matrix


def get_loaders(x_train, x_val, x_test, y_train, y_val, y_test, batch_size, device):
    """Return iterables over train, validation and test dataset"""

    # convert labels to vectors and put on device
    y_train = torch.from_numpy(np.asarray(y_train,dtype='int32')).to(device)
    y_val = torch.from_numpy(np.asarray(y_val,dtype='int32')).to(device)
    y_test = torch.from_numpy(np.asarray(y_test,dtype='int32')).to(device)

    # convert sequences of indexes to tensors and put on device
    x_train = torch.from_numpy(np.asarray(x_train, dtype='int32')).to(device)
    x_val = torch.from_numpy(np.asarray(x_val, dtype='int32')).to(device)
    x_test = torch.from_numpy(np.asarray(x_test, dtype='int32')).to(device)

    train_array = torch.utils.data.TensorDataset(x_train, y_train)
    #print(x_train.size(0), y_train.size(0))
    train_loader = torch.utils.data.DataLoader(train_array, batch_size)

    val_array = torch.utils.data.TensorDataset(x_val, y_val)
    val_loader = torch.utils.data.DataLoader(val_array, batch_size)

    test_array = torch.utils.data.TensorDataset(x_test, y_test)
    test_loader = torch.utils.data.DataLoader(test_array, batch_size)
    #print(x_train.size(0),y_train.size(0))
    return train_loader, val_loader, test_loader